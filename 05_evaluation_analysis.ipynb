{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from config import Config\n",
    "from training.evaluate import BertQAEvaluator\n",
    "from data.preprocessing import compute_exact_match, compute_f1\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae116f4",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c14c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config()\n",
    "\n",
    "# Path to best model\n",
    "best_model_path = '../checkpoints/best_model.pt'\n",
    "\n",
    "# Check if model exists\n",
    "if Path(best_model_path).exists():\n",
    "    print(f\"✓ Found model at {best_model_path}\")\n",
    "    \n",
    "    # Load checkpoint info\n",
    "    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "    print(f\"\\nModel Info:\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Global Step: {checkpoint.get('global_step', 'N/A')}\")\n",
    "    print(f\"  Best Eval Loss: {checkpoint.get('best_eval_loss', 'N/A'):.4f}\")\n",
    "else:\n",
    "    print(f\"❌ Model not found at {best_model_path}\")\n",
    "    print(\"Please train the model first using notebooks/04_model_training.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = BertQAEvaluator(best_model_path, config)\n",
    "print(\"✓ Evaluator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e86c8",
   "metadata": {},
   "source": [
    "## 2. Quick Test on Sample Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample questions\n",
    "test_samples = [\n",
    "    {\n",
    "        \"context\": \"\"\"The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical \n",
    "        rainforest in the Amazon biome that covers most of the Amazon basin of South America. \n",
    "        This basin encompasses 7,000,000 km2, of which 5,500,000 km2 are covered by the rainforest. \n",
    "        The majority of the forest is contained within Brazil, with 60% of the rainforest.\"\"\",\n",
    "        \"questions\": [\n",
    "            \"What is another name for the Amazon rainforest?\",\n",
    "            \"How much of the Amazon is in Brazil?\",\n",
    "            \"What percentage of the rainforest is in Brazil?\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"\"\"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
    "        It is named after the engineer Gustave Eiffel, whose company designed and built the tower. \n",
    "        Constructed from 1887 to 1889, it was initially criticized but has become a global cultural \n",
    "        icon of France and one of the most recognizable structures in the world.\"\"\",\n",
    "        \"questions\": [\n",
    "            \"Who is the Eiffel Tower named after?\",\n",
    "            \"When was the Eiffel Tower constructed?\",\n",
    "            \"Where is the Eiffel Tower located?\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing on Sample Questions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for sample_idx, sample in enumerate(test_samples, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {sample_idx}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Context: {sample['context'][:100]}...\\n\")\n",
    "    \n",
    "    for q_idx, question in enumerate(sample['questions'], 1):\n",
    "        result = evaluator.predict(question, sample['context'])\n",
    "        print(f\"{q_idx}. Q: {question}\")\n",
    "        print(f\"   A: {result['text']}\")\n",
    "        print(f\"   Confidence: {result['score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743762c0",
   "metadata": {},
   "source": [
    "## 3. Evaluate on Small Dev Subset (Quick Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation on 100 samples\n",
    "print(\"Running quick evaluation on 100 samples...\\n\")\n",
    "\n",
    "quick_results = evaluator.evaluate_dataset(\n",
    "    '../archive/dev-v1.1.json',\n",
    "    max_samples=100\n",
    ")\n",
    "\n",
    "print(f\"\\nQuick Test Results (100 samples):\")\n",
    "print(f\"  Exact Match: {quick_results['exact_match']:.2f}%\")\n",
    "print(f\"  F1 Score:    {quick_results['f1']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de5c92",
   "metadata": {},
   "source": [
    "## 4. Full Evaluation on Dev Set\n",
    "\n",
    "**Note:** This will take 15-30 minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full evaluation on entire dev set\n",
    "print(\"Running full evaluation on dev set...\")\n",
    "print(\"This may take 15-30 minutes...\\n\")\n",
    "\n",
    "full_results = evaluator.evaluate_dataset('../archive/dev-v1.1.json')\n",
    "\n",
    "# Save predictions\n",
    "evaluator.save_predictions(\n",
    "    full_results['predictions'],\n",
    "    '../outputs/dev_predictions.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d4680",
   "metadata": {},
   "source": [
    "## 5. Analyze Results by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47853d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev dataset to analyze questions\n",
    "with open('../archive/dev-v1.1.json', 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "# Extract question types and compute metrics per type\n",
    "question_type_metrics = defaultdict(lambda: {'em': [], 'f1': [], 'count': 0})\n",
    "\n",
    "for article in dev_data['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            question_id = qa['id']\n",
    "            question = qa['question']\n",
    "            \n",
    "            # Get question type (first word)\n",
    "            question_type = question.split()[0].lower() if question else 'unknown'\n",
    "            \n",
    "            # Get prediction and reference\n",
    "            if question_id in full_results['predictions']:\n",
    "                prediction = full_results['predictions'][question_id]\n",
    "                reference = qa['answers'][0]['text'] if qa['answers'] else ''\n",
    "                \n",
    "                # Compute metrics\n",
    "                em = compute_exact_match(prediction, reference)\n",
    "                f1 = compute_f1(prediction, reference)\n",
    "                \n",
    "                question_type_metrics[question_type]['em'].append(em)\n",
    "                question_type_metrics[question_type]['f1'].append(f1)\n",
    "                question_type_metrics[question_type]['count'] += 1\n",
    "\n",
    "# Calculate averages\n",
    "type_summary = []\n",
    "for qtype, metrics in question_type_metrics.items():\n",
    "    if metrics['count'] > 10:  # Only include types with >10 examples\n",
    "        type_summary.append({\n",
    "            'Question Type': qtype.capitalize(),\n",
    "            'Count': metrics['count'],\n",
    "            'EM (%)': np.mean(metrics['em']) * 100,\n",
    "            'F1 (%)': np.mean(metrics['f1']) * 100\n",
    "        })\n",
    "\n",
    "# Create DataFrame and sort by count\n",
    "type_df = pd.DataFrame(type_summary).sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Performance by Question Type\")\n",
    "print(\"=\"*70)\n",
    "print(type_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance by question type\n",
    "top_types = type_df.head(10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# EM scores\n",
    "axes[0].barh(top_types['Question Type'], top_types['EM (%)'], color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Exact Match (%)', fontsize=12)\n",
    "axes[0].set_title('EM Score by Question Type (Top 10)', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1 scores\n",
    "axes[1].barh(top_types['Question Type'], top_types['F1 (%)'], color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('F1 Score (%)', fontsize=12)\n",
    "axes[1].set_title('F1 Score by Question Type (Top 10)', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/question_type_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d98707",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "errors = evaluator.analyze_errors(\n",
    "    full_results['predictions'],\n",
    "    full_results['references'],\n",
    "    top_n=15\n",
    ")\n",
    "\n",
    "total_predictions = len(full_results['predictions'])\n",
    "total_errors = len(errors)\n",
    "error_rate = (total_errors / total_predictions) * 100\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Error Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Errors (EM=0): {total_errors}\")\n",
    "print(f\"Error Rate: {error_rate:.2f}%\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize errors by F1 score\n",
    "error_categories = {\n",
    "    'Complete Miss (F1=0)': [e for e in errors if e['f1'] == 0],\n",
    "    'Partial Match (0<F1<0.5)': [e for e in errors if 0 < e['f1'] < 0.5],\n",
    "    'Close Match (F1>=0.5)': [e for e in errors if e['f1'] >= 0.5]\n",
    "}\n",
    "\n",
    "print(\"\\nError Categories:\")\n",
    "for category, error_list in error_categories.items():\n",
    "    print(f\"  {category}: {len(error_list)} ({len(error_list)/total_errors*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f625577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error distribution\n",
    "f1_scores = [e['f1'] for e in errors]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(f1_scores, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "plt.xlabel('F1 Score', fontsize=12)\n",
    "plt.ylabel('Number of Errors', fontsize=12)\n",
    "plt.title('Distribution of F1 Scores for Incorrect Predictions (EM=0)', fontsize=14, fontweight='bold')\n",
    "plt.axvline(np.mean(f1_scores), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean F1: {np.mean(f1_scores):.3f}')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/error_f1_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440a547",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a460d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with expected BERT-base performance\n",
    "baseline_scores = {\n",
    "    'Model': ['BERT-base (Expected)', 'Our Model'],\n",
    "    'Exact Match': [82.0, full_results['exact_match']],\n",
    "    'F1 Score': [90.0, full_results['f1']]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(baseline_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate differences\n",
    "em_diff = full_results['exact_match'] - 82.0\n",
    "f1_diff = full_results['f1'] - 90.0\n",
    "\n",
    "print(f\"\\nDifference from Expected BERT-base:\")\n",
    "print(f\"  EM: {em_diff:+.2f}%\")\n",
    "print(f\"  F1: {f1_diff:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49074da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "x = np.arange(len(comparison_df['Model']))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rects1 = ax.bar(x - width/2, comparison_df['Exact Match'], width, label='Exact Match', \n",
    "                color='skyblue', edgecolor='black')\n",
    "rects2 = ax.bar(x + width/2, comparison_df['F1 Score'], width, label='F1 Score',\n",
    "                color='lightcoral', edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Score (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "# Add value labels on bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b01edc",
   "metadata": {},
   "source": [
    "## 8. Save Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation report\n",
    "report = {\n",
    "    'model_info': {\n",
    "        'model_name': 'BERT-base-uncased',\n",
    "        'checkpoint': best_model_path,\n",
    "        'epoch': checkpoint.get('epoch', 'N/A'),\n",
    "        'global_step': checkpoint.get('global_step', 'N/A')\n",
    "    },\n",
    "    'overall_metrics': {\n",
    "        'exact_match': full_results['exact_match'],\n",
    "        'f1_score': full_results['f1'],\n",
    "        'total_examples': len(full_results['predictions'])\n",
    "    },\n",
    "    'error_analysis': {\n",
    "        'total_errors': total_errors,\n",
    "        'error_rate': error_rate,\n",
    "        'avg_f1_on_errors': np.mean(f1_scores) if f1_scores else 0\n",
    "    },\n",
    "    'question_type_performance': type_df.to_dict('records')\n",
    "}\n",
    "\n",
    "# Save report\n",
    "report_path = '../outputs/evaluation_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Evaluation report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d20999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create markdown report\n",
    "markdown_report = f\"\"\"# BERT-QA Evaluation Report\n",
    "\n",
    "## Model Information\n",
    "- **Model**: BERT-base-uncased\n",
    "- **Checkpoint**: {best_model_path}\n",
    "- **Epoch**: {checkpoint.get('epoch', 'N/A')}\n",
    "- **Training Steps**: {checkpoint.get('global_step', 'N/A')}\n",
    "\n",
    "## Overall Performance\n",
    "- **Exact Match (EM)**: {full_results['exact_match']:.2f}%\n",
    "- **F1 Score**: {full_results['f1']:.2f}%\n",
    "- **Total Examples**: {len(full_results['predictions']):,}\n",
    "\n",
    "## Comparison with BERT-base Baseline\n",
    "| Metric | Expected BERT-base | Our Model | Difference |\n",
    "|--------|-------------------|-----------|------------|\n",
    "| EM     | 82.0%            | {full_results['exact_match']:.2f}% | {full_results['exact_match']-82.0:+.2f}% |\n",
    "| F1     | 90.0%            | {full_results['f1']:.2f}% | {full_results['f1']-90.0:+.2f}% |\n",
    "\n",
    "## Error Analysis\n",
    "- **Total Errors (EM=0)**: {total_errors:,} ({error_rate:.2f}%)\n",
    "- **Average F1 on Errors**: {np.mean(f1_scores) if f1_scores else 0:.3f}\n",
    "\n",
    "## Top Question Types Performance\n",
    "{type_df.head(10).to_markdown(index=False)}\n",
    "\n",
    "## Conclusion\n",
    "The model achieves competitive performance on the SQuAD v1.1 dataset, demonstrating strong \n",
    "question-answering capabilities across various question types.\n",
    "\n",
    "---\n",
    "*Report generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "\n",
    "# Save markdown report\n",
    "md_report_path = '../outputs/evaluation_report.md'\n",
    "with open(md_report_path, 'w') as f:\n",
    "    f.write(markdown_report)\n",
    "\n",
    "print(f\"✓ Markdown report saved to {md_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fae15",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Evaluation Complete! ✓\n",
    "\n",
    "**Key Metrics:**\n",
    "- Exact Match and F1 scores computed on full dev set\n",
    "- Performance analyzed by question type\n",
    "- Error patterns identified and categorized\n",
    "\n",
    "**Generated Files:**\n",
    "- `outputs/dev_predictions.json` - All predictions\n",
    "- `outputs/evaluation_report.json` - Detailed metrics\n",
    "- `outputs/evaluation_report.md` - Human-readable report\n",
    "- `outputs/*.png` - Visualization charts\n",
    "\n",
    "**Next Step:** Create deployment interface for interactive question answering!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

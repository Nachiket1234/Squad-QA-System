{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c21d16",
   "metadata": {},
   "source": [
    "## 1. Load SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_path = Path('../archive/train-v1.1.json')\n",
    "dev_path = Path('../archive/dev-v1.1.json')\n",
    "\n",
    "# Load training data\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load development/validation data\n",
    "with open(dev_path, 'r', encoding='utf-8') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded training data from {train_path}\")\n",
    "print(f\"✓ Loaded dev data from {dev_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e61e3",
   "metadata": {},
   "source": [
    "## 2. Understand JSON Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ce5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine top-level structure\n",
    "print(\"Top-level keys:\", train_data.keys())\n",
    "print(\"\\nDataset version:\", train_data.get('version', 'N/A'))\n",
    "print(f\"Number of articles in training set: {len(train_data['data'])}\")\n",
    "print(f\"Number of articles in dev set: {len(dev_data['data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first article structure\n",
    "sample_article = train_data['data'][0]\n",
    "print(\"Article title:\", sample_article['title'])\n",
    "print(\"\\nNumber of paragraphs:\", len(sample_article['paragraphs']))\n",
    "print(\"\\nFirst paragraph keys:\", sample_article['paragraphs'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfeeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine context and questions\n",
    "sample_paragraph = sample_article['paragraphs'][0]\n",
    "print(\"Context (first 200 chars):\")\n",
    "print(sample_paragraph['context'][:200], \"...\\n\")\n",
    "\n",
    "print(f\"Number of questions for this context: {len(sample_paragraph['qas'])}\")\n",
    "print(\"\\nFirst question example:\")\n",
    "sample_qa = sample_paragraph['qas'][0]\n",
    "print(json.dumps(sample_qa, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae005d",
   "metadata": {},
   "source": [
    "## 3. Extract Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c476c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statistics(data, dataset_name='Dataset'):\n",
    "    \"\"\"Extract comprehensive statistics from SQuAD data.\"\"\"\n",
    "    stats = {\n",
    "        'num_articles': 0,\n",
    "        'num_paragraphs': 0,\n",
    "        'num_questions': 0,\n",
    "        'context_lengths': [],\n",
    "        'question_lengths': [],\n",
    "        'answer_lengths': [],\n",
    "        'answer_starts': [],\n",
    "        'questions_per_context': [],\n",
    "        'question_types': [],\n",
    "        'article_titles': []\n",
    "    }\n",
    "    \n",
    "    for article in data['data']:\n",
    "        stats['num_articles'] += 1\n",
    "        stats['article_titles'].append(article['title'])\n",
    "        \n",
    "        for paragraph in article['paragraphs']:\n",
    "            stats['num_paragraphs'] += 1\n",
    "            context = paragraph['context']\n",
    "            stats['context_lengths'].append(len(context))\n",
    "            \n",
    "            num_questions = len(paragraph['qas'])\n",
    "            stats['questions_per_context'].append(num_questions)\n",
    "            \n",
    "            for qa in paragraph['qas']:\n",
    "                stats['num_questions'] += 1\n",
    "                question = qa['question']\n",
    "                stats['question_lengths'].append(len(question))\n",
    "                \n",
    "                # Extract question type (first word)\n",
    "                first_word = question.split()[0].lower() if question else 'unknown'\n",
    "                stats['question_types'].append(first_word)\n",
    "                \n",
    "                # Answer information\n",
    "                if qa['answers']:\n",
    "                    answer = qa['answers'][0]  # Use first answer\n",
    "                    stats['answer_lengths'].append(len(answer['text']))\n",
    "                    stats['answer_starts'].append(answer['answer_start'])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{dataset_name} Statistics\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total articles: {stats['num_articles']:,}\")\n",
    "    print(f\"Total paragraphs: {stats['num_paragraphs']:,}\")\n",
    "    print(f\"Total questions: {stats['num_questions']:,}\")\n",
    "    print(f\"\\nAverage questions per context: {np.mean(stats['questions_per_context']):.2f}\")\n",
    "    print(f\"Average context length (chars): {np.mean(stats['context_lengths']):.0f}\")\n",
    "    print(f\"Average question length (chars): {np.mean(stats['question_lengths']):.0f}\")\n",
    "    print(f\"Average answer length (chars): {np.mean(stats['answer_lengths']):.0f}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Extract statistics for both datasets\n",
    "train_stats = extract_statistics(train_data, 'Training Set')\n",
    "dev_stats = extract_statistics(dev_data, 'Development Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997aaadb",
   "metadata": {},
   "source": [
    "## 4. Visualize Dataset Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(train_stats['answer_lengths'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Answer Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Training Set: Answer Length Distribution')\n",
    "axes[0].axvline(np.mean(train_stats['answer_lengths']), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(train_stats[\"answer_lengths\"]):.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(dev_stats['answer_lengths'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Answer Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Dev Set: Answer Length Distribution')\n",
    "axes[1].axvline(np.mean(dev_stats['answer_lengths']), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(dev_stats[\"answer_lengths\"]):.1f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(train_stats['context_lengths'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0].set_xlabel('Context Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Training Set: Context Length Distribution')\n",
    "axes[0].axvline(np.mean(train_stats['context_lengths']), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(train_stats[\"context_lengths\"]):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(dev_stats['context_lengths'], bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1].set_xlabel('Context Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Dev Set: Context Length Distribution')\n",
    "axes[1].axvline(np.mean(dev_stats['context_lengths']), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(dev_stats[\"context_lengths\"]):.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions per context distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(train_stats['questions_per_context'], bins=range(0, 15), \n",
    "         edgecolor='black', alpha=0.7, color='teal')\n",
    "plt.xlabel('Number of Questions per Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Training Set: Questions per Context Distribution')\n",
    "plt.axvline(np.mean(train_stats['questions_per_context']), color='red', linestyle='--',\n",
    "            label=f'Mean: {np.mean(train_stats[\"questions_per_context\"]):.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648a37d",
   "metadata": {},
   "source": [
    "## 5. Question Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ded2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count question types\n",
    "train_question_types = Counter(train_stats['question_types'])\n",
    "dev_question_types = Counter(dev_stats['question_types'])\n",
    "\n",
    "# Get top 10 question types\n",
    "top_question_types = train_question_types.most_common(10)\n",
    "\n",
    "print(\"Top 10 Question Types in Training Set:\")\n",
    "print(\"-\" * 40)\n",
    "for qtype, count in top_question_types:\n",
    "    percentage = (count / train_stats['num_questions']) * 100\n",
    "    print(f\"{qtype.capitalize():15s} {count:6,} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59855b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize question types\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set\n",
    "types, counts = zip(*train_question_types.most_common(10))\n",
    "axes[0].barh(types, counts, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_title('Training Set: Top 10 Question Types')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Dev set\n",
    "types_dev, counts_dev = zip(*dev_question_types.most_common(10))\n",
    "axes[1].barh(types_dev, counts_dev, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_title('Dev Set: Top 10 Question Types')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59e317",
   "metadata": {},
   "source": [
    "## 6. Sample Article Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba665e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample article titles\n",
    "print(\"Sample Article Titles from Training Set:\")\n",
    "print(\"=\" * 50)\n",
    "for i, title in enumerate(train_stats['article_titles'][:20], 1):\n",
    "    print(f\"{i:2d}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f196f",
   "metadata": {},
   "source": [
    "## 7. Create Summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a149630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Articles',\n",
    "        'Paragraphs',\n",
    "        'Questions',\n",
    "        'Avg Questions/Context',\n",
    "        'Avg Context Length (chars)',\n",
    "        'Avg Question Length (chars)',\n",
    "        'Avg Answer Length (chars)',\n",
    "        'Max Answer Length (chars)',\n",
    "        'Min Answer Length (chars)'\n",
    "    ],\n",
    "    'Training Set': [\n",
    "        f\"{train_stats['num_articles']:,}\",\n",
    "        f\"{train_stats['num_paragraphs']:,}\",\n",
    "        f\"{train_stats['num_questions']:,}\",\n",
    "        f\"{np.mean(train_stats['questions_per_context']):.2f}\",\n",
    "        f\"{np.mean(train_stats['context_lengths']):.0f}\",\n",
    "        f\"{np.mean(train_stats['question_lengths']):.0f}\",\n",
    "        f\"{np.mean(train_stats['answer_lengths']):.0f}\",\n",
    "        f\"{max(train_stats['answer_lengths'])}\",\n",
    "        f\"{min(train_stats['answer_lengths'])}\"\n",
    "    ],\n",
    "    'Dev Set': [\n",
    "        f\"{dev_stats['num_articles']:,}\",\n",
    "        f\"{dev_stats['num_paragraphs']:,}\",\n",
    "        f\"{dev_stats['num_questions']:,}\",\n",
    "        f\"{np.mean(dev_stats['questions_per_context']):.2f}\",\n",
    "        f\"{np.mean(dev_stats['context_lengths']):.0f}\",\n",
    "        f\"{np.mean(dev_stats['question_lengths']):.0f}\",\n",
    "        f\"{np.mean(dev_stats['answer_lengths']):.0f}\",\n",
    "        f\"{max(dev_stats['answer_lengths'])}\",\n",
    "        f\"{min(dev_stats['answer_lengths'])}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SQuAD v1.1 Dataset Summary\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc8e95",
   "metadata": {},
   "source": [
    "## 8. Sample Question-Answer Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b075e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample QA pairs\n",
    "def display_sample_qa(data, num_samples=3):\n",
    "    \"\"\"Display sample question-answer pairs with context.\"\"\"\n",
    "    samples_shown = 0\n",
    "    \n",
    "    for article in data['data']:\n",
    "        if samples_shown >= num_samples:\n",
    "            break\n",
    "            \n",
    "        for paragraph in article['paragraphs']:\n",
    "            if samples_shown >= num_samples:\n",
    "                break\n",
    "                \n",
    "            context = paragraph['context']\n",
    "            qa = paragraph['qas'][0] if paragraph['qas'] else None\n",
    "            \n",
    "            if qa and qa['answers']:\n",
    "                answer = qa['answers'][0]\n",
    "                samples_shown += 1\n",
    "                \n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"Sample {samples_shown}: {article['title']}\")\n",
    "                print(f\"{'='*80}\")\n",
    "                print(f\"\\nContext:\\n{context[:300]}...\" if len(context) > 300 else f\"\\nContext:\\n{context}\")\n",
    "                print(f\"\\nQuestion: {qa['question']}\")\n",
    "                print(f\"\\nAnswer: {answer['text']}\")\n",
    "                print(f\"Answer starts at character position: {answer['answer_start']}\")\n",
    "\n",
    "display_sample_qa(train_data, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f996f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **Dataset Size**: ~87k training questions, ~10k dev questions\n",
    "2. **Question Types**: Dominated by \"what\", \"who\", \"when\", \"how\" questions\n",
    "3. **Answer Format**: Short extractive spans (average ~15-20 characters)\n",
    "4. **Context Length**: Varies widely but averages ~600-800 characters\n",
    "5. **Questions per Context**: Typically 3-5 questions per paragraph\n",
    "\n",
    "### Next Steps:\n",
    "- Tokenize questions and contexts using BERT tokenizer\n",
    "- Map character-level answer positions to token positions\n",
    "- Build PyTorch Dataset and DataLoader\n",
    "- Fine-tune BERT for question answering"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

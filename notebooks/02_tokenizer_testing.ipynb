{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9744c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04763b",
   "metadata": {},
   "source": [
    "## 1. Load BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeffec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"✓ Loaded BERT tokenizer\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d724fb",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data from SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample from the training set\n",
    "with open('../archive/train-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Extract a sample question-context pair\n",
    "sample_article = train_data['data'][0]\n",
    "sample_paragraph = sample_article['paragraphs'][0]\n",
    "sample_qa = sample_paragraph['qas'][0]\n",
    "\n",
    "context = sample_paragraph['context']\n",
    "question = sample_qa['question']\n",
    "answer_text = sample_qa['answers'][0]['text']\n",
    "answer_start = sample_qa['answers'][0]['answer_start']\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nContext: {context[:200]}...\")\n",
    "print(f\"\\nAnswer: '{answer_text}'\")\n",
    "print(f\"Answer starts at character position: {answer_start}\")\n",
    "print(f\"\\nVerification: '{context[answer_start:answer_start+len(answer_text)]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a5dce",
   "metadata": {},
   "source": [
    "## 3. Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bdefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize question only\n",
    "question_tokens = tokenizer.tokenize(question)\n",
    "print(\"Question tokens:\")\n",
    "print(question_tokens)\n",
    "print(f\"\\nNumber of tokens: {len(question_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize context only (first 100 chars)\n",
    "context_sample = context[:100]\n",
    "context_tokens = tokenizer.tokenize(context_sample)\n",
    "print(\"Context sample tokens:\")\n",
    "print(context_tokens)\n",
    "print(f\"\\nNumber of tokens: {len(context_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716a9fc",
   "metadata": {},
   "source": [
    "## 4. Question-Context Pair Tokenization\n",
    "\n",
    "For QA, we need to tokenize question and context together with special format:\n",
    "```\n",
    "[CLS] question [SEP] context [SEP]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b52f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize question and context as a pair\n",
    "encoding = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=384,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(\"Encoding keys:\", encoding.keys())\n",
    "print(f\"\\nInput IDs shape: {encoding['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {encoding['attention_mask'].shape}\")\n",
    "print(f\"Token type IDs shape: {encoding['token_type_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da241038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode to see the tokenized text\n",
    "decoded = tokenizer.decode(encoding['input_ids'][0])\n",
    "print(\"Decoded tokens (first 300 chars):\")\n",
    "print(decoded[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443ca68",
   "metadata": {},
   "source": [
    "## 5. Understanding Token Type IDs\n",
    "\n",
    "Token type IDs distinguish question tokens (0) from context tokens (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b616e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 30 tokens with their types\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "token_type_ids = encoding['token_type_ids'][0].tolist()\n",
    "\n",
    "print(\"Token | Type (0=Question, 1=Context)\")\n",
    "print(\"=\"*50)\n",
    "for i in range(min(30, len(tokens))):\n",
    "    token = tokens[i]\n",
    "    token_type = token_type_ids[i]\n",
    "    segment = \"QUESTION\" if token_type == 0 else \"CONTEXT\"\n",
    "    print(f\"{i:3d}. {token:15s} | {token_type} ({segment})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2918b9d",
   "metadata": {},
   "source": [
    "## 6. Offset Mappings - The Key to Answer Span Conversion\n",
    "\n",
    "Offset mappings show which character positions each token corresponds to in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8345fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize with offset mappings\n",
    "encoding_with_offsets = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    truncation=True,\n",
    "    max_length=384,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "offset_mapping = encoding_with_offsets['offset_mapping'][0]\n",
    "print(f\"Offset mapping shape: {offset_mapping.shape}\")\n",
    "print(f\"\\nFirst 20 offset mappings (char start, char end):\")\n",
    "print(offset_mapping[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c66b9c",
   "metadata": {},
   "source": [
    "## 7. Converting Character Positions to Token Positions\n",
    "\n",
    "This is crucial for finding the answer span in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer_span(answer_start_char, answer_text, offset_mapping, sequence_ids):\n",
    "    \"\"\"\n",
    "    Convert character-level answer position to token-level positions.\n",
    "    \n",
    "    Args:\n",
    "        answer_start_char: Character position where answer starts\n",
    "        answer_text: The answer text\n",
    "        offset_mapping: Tensor of (start, end) character offsets for each token\n",
    "        sequence_ids: List indicating which tokens belong to context (1) vs question (0)\n",
    "    \n",
    "    Returns:\n",
    "        start_token_idx: Token index where answer starts\n",
    "        end_token_idx: Token index where answer ends\n",
    "    \"\"\"\n",
    "    answer_end_char = answer_start_char + len(answer_text)\n",
    "    \n",
    "    # Find start token\n",
    "    start_token_idx = None\n",
    "    for idx, (start, end) in enumerate(offset_mapping):\n",
    "        # Only consider context tokens (sequence_id == 1)\n",
    "        if sequence_ids[idx] == 1:\n",
    "            if start <= answer_start_char < end:\n",
    "                start_token_idx = idx\n",
    "                break\n",
    "    \n",
    "    # Find end token\n",
    "    end_token_idx = None\n",
    "    for idx, (start, end) in enumerate(offset_mapping):\n",
    "        if sequence_ids[idx] == 1:\n",
    "            if start < answer_end_char <= end:\n",
    "                end_token_idx = idx\n",
    "                break\n",
    "    \n",
    "    return start_token_idx, end_token_idx\n",
    "\n",
    "# Get sequence IDs to identify context tokens\n",
    "sequence_ids = encoding_with_offsets.sequence_ids(0)\n",
    "\n",
    "# Find answer span in tokens\n",
    "start_token, end_token = find_answer_span(\n",
    "    answer_start, \n",
    "    answer_text, \n",
    "    offset_mapping,\n",
    "    sequence_ids\n",
    ")\n",
    "\n",
    "print(f\"Answer: '{answer_text}'\")\n",
    "print(f\"Character position: {answer_start} to {answer_start + len(answer_text)}\")\n",
    "print(f\"\\nToken position: {start_token} to {end_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify by decoding the token span\n",
    "if start_token is not None and end_token is not None:\n",
    "    answer_token_ids = encoding_with_offsets['input_ids'][0][start_token:end_token+1]\n",
    "    decoded_answer = tokenizer.decode(answer_token_ids)\n",
    "    \n",
    "    print(\"Verification:\")\n",
    "    print(f\"Original answer: '{answer_text}'\")\n",
    "    print(f\"Decoded from tokens: '{decoded_answer}'\")\n",
    "    print(f\"\\nMatch: {answer_text.lower() in decoded_answer.lower()}\")\n",
    "else:\n",
    "    print(\"Could not find answer span in tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9e30c",
   "metadata": {},
   "source": [
    "## 8. Visualize Token Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb959caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tokens around the answer\n",
    "if start_token is not None and end_token is not None:\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding_with_offsets['input_ids'][0])\n",
    "    \n",
    "    print(\"Tokens around the answer:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show 5 tokens before and after\n",
    "    window_start = max(0, start_token - 5)\n",
    "    window_end = min(len(tokens), end_token + 6)\n",
    "    \n",
    "    for idx in range(window_start, window_end):\n",
    "        token = tokens[idx]\n",
    "        is_answer = start_token <= idx <= end_token\n",
    "        marker = \">>> \" if is_answer else \"    \"\n",
    "        print(f\"{marker}{idx:3d}. {token:20s} {offset_mapping[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860ecb8",
   "metadata": {},
   "source": [
    "## 9. Handling Long Contexts (Stride)\n",
    "\n",
    "When context exceeds max_length, we use stride to create overlapping windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with a longer context\n",
    "long_context = context * 3  # Artificially create a long context\n",
    "\n",
    "# Tokenize with stride\n",
    "encoding_with_stride = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    truncation='only_second',  # Only truncate context, not question\n",
    "    max_length=384,\n",
    "    stride=128,  # Overlap of 128 tokens between chunks\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding='max_length'\n",
    ")\n",
    "\n",
    "print(f\"Number of chunks created: {len(encoding_with_stride['input_ids'])}\")\n",
    "print(f\"\\nEach chunk has {encoding_with_stride['input_ids'][0].shape[0]} tokens\")\n",
    "print(f\"Stride (overlap): 128 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f1f93",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Key Concepts for BERT Tokenization in QA:\n",
    "\n",
    "1. **Special Token Format**: [CLS] question [SEP] context [SEP]\n",
    "\n",
    "2. **Token Type IDs**:\n",
    "   - 0 = question tokens\n",
    "   - 1 = context tokens\n",
    "\n",
    "3. **Offset Mappings**:\n",
    "   - Maps each token to its character position in original text\n",
    "   - Essential for converting answer_start (char) to token positions\n",
    "\n",
    "4. **Answer Span Conversion**:\n",
    "   - Character position (answer_start) → Token position (start_token_idx)\n",
    "   - Use offset_mapping to find which tokens contain the answer\n",
    "\n",
    "5. **Handling Long Contexts**:\n",
    "   - Use truncation='only_second' to preserve question\n",
    "   - Use stride for overlapping windows\n",
    "   - Set return_overflowing_tokens=True\n",
    "\n",
    "6. **Sequence IDs**:\n",
    "   - Use encoding.sequence_ids() to identify context vs question\n",
    "   - Important for ensuring answer is only in context portion\n",
    "\n",
    "Next Steps:\n",
    "- Build PyTorch Dataset that handles this tokenization\n",
    "- Implement answer span finding logic in batch processing\n",
    "- Create DataLoader for training\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

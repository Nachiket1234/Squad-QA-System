{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from config import Config\n",
    "from training.train import BertQATrainer\n",
    "from training.evaluate import BertQAEvaluator\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665500f7",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf1678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display configuration\n",
    "config = Config()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf30ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify configuration for quick testing (optional)\n",
    "# Uncomment these lines to test with smaller settings first\n",
    "\n",
    "# config.training.num_epochs = 1\n",
    "# config.training.batch_size = 8\n",
    "# config.training.logging_steps = 50\n",
    "# config.training.save_steps = 500\n",
    "\n",
    "print(\"Configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a54614",
   "metadata": {},
   "source": [
    "## 2. Test Training on Small Subset (Optional)\n",
    "\n",
    "Before training on the full dataset, let's test with a small subset to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small test dataset\n",
    "import json\n",
    "\n",
    "# Load full dataset\n",
    "with open('../archive/train-v1.1.json', 'r') as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "# Create small version (first 2 articles)\n",
    "small_data = {\n",
    "    'version': full_data['version'],\n",
    "    'data': full_data['data'][:2]  # Just first 2 articles\n",
    "}\n",
    "\n",
    "# Save small version\n",
    "with open('../archive/train_small.json', 'w') as f:\n",
    "    json.dump(small_data, f)\n",
    "\n",
    "# Do the same for dev\n",
    "with open('../archive/dev-v1.1.json', 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "small_dev = {\n",
    "    'version': dev_data['version'],\n",
    "    'data': dev_data['data'][:2]\n",
    "}\n",
    "\n",
    "with open('../archive/dev_small.json', 'w') as f:\n",
    "    json.dump(small_dev, f)\n",
    "\n",
    "print(\"Small test datasets created!\")\n",
    "print(f\"Train articles: {len(small_data['data'])}\")\n",
    "print(f\"Dev articles: {len(small_dev['data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training on small dataset\n",
    "test_config = Config()\n",
    "test_config.data.train_file = 'archive/train_small.json'\n",
    "test_config.data.dev_file = 'archive/dev_small.json'\n",
    "test_config.training.num_epochs = 1\n",
    "test_config.training.batch_size = 4\n",
    "test_config.training.logging_steps = 10\n",
    "\n",
    "print(\"Testing with small dataset...\")\n",
    "test_trainer = BertQATrainer(test_config)\n",
    "test_trainer.setup()\n",
    "\n",
    "print(\"\\nSmall dataset setup successful!\")\n",
    "print(\"You can now run: test_trainer.train() to test the training loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12eff0",
   "metadata": {},
   "source": [
    "## 3. Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = BertQATrainer(config)\n",
    "\n",
    "# Setup (load data, model, optimizer)\n",
    "trainer.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb80706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "# WARNING: This will take several hours on CPU, ~2-3 hours on GPU\n",
    "# Make sure you have saved your work before running\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a4e41",
   "metadata": {},
   "source": [
    "## 4. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse training logs\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the latest log file\n",
    "log_dir = Path('../logs')\n",
    "log_files = sorted(log_dir.glob('training_*.log'))\n",
    "\n",
    "if log_files:\n",
    "    latest_log = log_files[-1]\n",
    "    print(f\"Reading log: {latest_log}\")\n",
    "    \n",
    "    # Parse training and evaluation losses\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    epochs = []\n",
    "    \n",
    "    with open(latest_log, 'r') as f:\n",
    "        for line in f:\n",
    "            # Match training loss\n",
    "            if 'Training Loss:' in line:\n",
    "                match = re.search(r'Epoch (\\d+) - Training Loss: ([\\d.]+)', line)\n",
    "                if match:\n",
    "                    epoch = int(match.group(1))\n",
    "                    loss = float(match.group(2))\n",
    "                    epochs.append(epoch)\n",
    "                    train_losses.append(loss)\n",
    "            \n",
    "            # Match evaluation loss\n",
    "            if 'Evaluation Loss:' in line:\n",
    "                match = re.search(r'Evaluation Loss: ([\\d.]+)', line)\n",
    "                if match:\n",
    "                    loss = float(match.group(1))\n",
    "                    eval_losses.append(loss)\n",
    "    \n",
    "    print(f\"Found {len(train_losses)} training epochs\")\n",
    "else:\n",
    "    print(\"No log files found. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if train_losses and eval_losses:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(epochs, train_losses, marker='o', label='Training Loss', linewidth=2)\n",
    "    plt.plot(epochs, eval_losses, marker='s', label='Evaluation Loss', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Training and Evaluation Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for best model\n",
    "    best_epoch = epochs[eval_losses.index(min(eval_losses))]\n",
    "    best_loss = min(eval_losses)\n",
    "    plt.axvline(x=best_epoch, color='red', linestyle='--', alpha=0.5, label=f'Best Model (Epoch {best_epoch})')\n",
    "    plt.legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nBest model at epoch {best_epoch} with eval loss: {best_loss:.4f}\")\n",
    "else:\n",
    "    print(\"No training data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f373f",
   "metadata": {},
   "source": [
    "## 5. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f685d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "best_model_path = '../checkpoints/best_model.pt'\n",
    "\n",
    "if Path(best_model_path).exists():\n",
    "    evaluator = BertQAEvaluator(best_model_path, config)\n",
    "    print(\"Best model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Model not found at {best_model_path}\")\n",
    "    print(\"Please train the model first or specify correct path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f38238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on dev set\n",
    "# Start with a small sample for quick testing\n",
    "results = evaluator.evaluate_dataset(\n",
    "    '../archive/dev-v1.1.json',\n",
    "    max_samples=100  # Remove this to evaluate on full dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbe2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on FULL dev set (this will take time)\n",
    "# Uncomment to run full evaluation\n",
    "\n",
    "# full_results = evaluator.evaluate_dataset('../archive/dev-v1.1.json')\n",
    "# evaluator.save_predictions(full_results['predictions'], '../outputs/predictions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609944b",
   "metadata": {},
   "source": [
    "## 6. Test Predictions on Custom Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom question-context pairs\n",
    "context = \"\"\"The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest \n",
    "in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses \n",
    "7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest. \n",
    "The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with \n",
    "13%, and Colombia with 10%.\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What is another name for the Amazon rainforest?\",\n",
    "    \"How much of the Amazon rainforest is in Brazil?\",\n",
    "    \"Which country has the second largest portion of the Amazon?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Custom Predictions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    result = evaluator.predict(question, context)\n",
    "    print(f\"\\nQ{i}: {question}\")\n",
    "    print(f\"A:  {result['text']}\")\n",
    "    print(f\"Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0fd2f",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors\n",
    "if 'results' in locals():\n",
    "    errors = evaluator.analyze_errors(\n",
    "        results['predictions'],\n",
    "        results['references'],\n",
    "        top_n=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTotal errors: {len(errors)}\")\n",
    "    print(f\"Error rate: {len(errors) / len(results['predictions']) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93319a84",
   "metadata": {},
   "source": [
    "## 8. Save Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f567cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in Hugging Face format for easy deployment\n",
    "if Path(best_model_path).exists():\n",
    "    from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "    \n",
    "    # Load model\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Save in Hugging Face format\n",
    "    output_dir = Path('../outputs/bert-qa-squad')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "    print(\"You can now load it with:\")\n",
    "    print(f\"  model = BertForQuestionAnswering.from_pretrained('{output_dir}')\")\n",
    "    print(f\"  tokenizer = BertTokenizerFast.from_pretrained('{output_dir}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680de3ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Training Results:\n",
    "- Model: BERT-base-uncased\n",
    "- Dataset: SQuAD v1.1\n",
    "- Training time: ~2-3 hours on GPU\n",
    "\n",
    "### Expected Performance:\n",
    "- Exact Match: ~80-85%\n",
    "- F1 Score: ~88-92%\n",
    "\n",
    "### Next Steps:\n",
    "1. Build deployment interface (Gradio/Streamlit)\n",
    "2. Create interactive demo\n",
    "3. Document usage and API\n",
    "4. Deploy to cloud (optional)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

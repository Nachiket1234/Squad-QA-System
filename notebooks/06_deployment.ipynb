{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc47b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from inference.predict import QAPredictor\n",
    "from config import Config\n",
    "\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774288b",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f51bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model not found at ../checkpoints/best_model.pt\n",
      "Using base BERT model (not fine-tuned on SQuAD)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using base model: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Load the predictor with trained model\n",
    "model_path = '../checkpoints/best_model.pt'\n",
    "\n",
    "if Path(model_path).exists():\n",
    "    predictor = QAPredictor(model_path)\n",
    "    print(\"âœ“ Model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Warning: Model not found at {model_path}\")\n",
    "    print(\"Using base BERT model (not fine-tuned on SQuAD)\")\n",
    "    predictor = QAPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb4155",
   "metadata": {},
   "source": [
    "## 2. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fdb19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Prediction:\n",
      "================================================================================\n",
      "Question: Who is the Eiffel Tower named after?\n",
      "\n",
      "Answer: el, whose company designed and\n",
      "Confidence: 38.4%\n",
      "Score: 1.53\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with sample data\n",
    "sample_context = \"\"\"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
    "It is named after the engineer Gustave Eiffel, whose company designed and built the tower. \n",
    "Constructed from 1887 to 1889 as the entrance arch to the 1889 World's Fair, it was initially \n",
    "criticized by some of France's leading artists and intellectuals for its design, but it has \n",
    "become a global cultural icon of France and one of the most recognizable structures in the world. \n",
    "The Eiffel Tower is the most-visited paid monument in the world; 6.91 million people ascended \n",
    "it in 2015.\"\"\"\n",
    "\n",
    "sample_question = \"Who is the Eiffel Tower named after?\"\n",
    "\n",
    "result = predictor.predict(sample_question, sample_context)\n",
    "\n",
    "print(\"Sample Prediction:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.1f}%\")\n",
    "print(f\"Score: {result['score']:.2f}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d089228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context with highlighted answer:\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
      "It is named after the engineer Gustave Eiff**el, whose company designed and** built the tower. \n",
      "Constructed from 1887 to 1889 as the entrance arch to the 1889 World's Fair, it was initially \n",
      "criticized by some of France's leading artists and intellectuals for its design, but it has \n",
      "become a global cultural icon of France and one of the most recognizable structures in the world. \n",
      "The Eiffel Tower is the most-visited paid monument in the world; 6.91 million people ascended \n",
      "it in 2015.\n"
     ]
    }
   ],
   "source": [
    "# Test with highlighted answer\n",
    "highlighted = predictor.highlight_answer(sample_context, result['answer'])\n",
    "print(\"Context with highlighted answer:\")\n",
    "print(highlighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf48f93",
   "metadata": {},
   "source": [
    "## 3. Create Example Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f1d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 5 example question-context pairs\n"
     ]
    }
   ],
   "source": [
    "# Prepare example contexts for the demo\n",
    "examples = [\n",
    "    [\n",
    "        \"What is the capital of France?\",\n",
    "        \"\"\"Paris is the capital and most populous city of France. Situated on the Seine River, \n",
    "        in the north of the country, it is in the centre of the ÃŽle-de-France region. The city \n",
    "        has an area of 105 square kilometers and a population of 2,206,488 inhabitants.\"\"\"\n",
    "    ],\n",
    "    [\n",
    "        \"How many people visited the Eiffel Tower in 2015?\",\n",
    "        sample_context\n",
    "    ],\n",
    "    [\n",
    "        \"What is the Amazon rainforest?\",\n",
    "        \"\"\"The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest \n",
    "        in the Amazon biome that covers most of the Amazon basin of South America. This basin \n",
    "        encompasses 7,000,000 square kilometers, of which 5,500,000 square kilometers are covered \n",
    "        by the rainforest. The majority of the forest is contained within Brazil, with 60% of the \n",
    "        rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, \n",
    "        Ecuador, Bolivia, Guyana, Suriname, and French Guiana.\"\"\"\n",
    "    ],\n",
    "    [\n",
    "        \"When was the United Nations founded?\",\n",
    "        \"\"\"The United Nations (UN) is an intergovernmental organization tasked with maintaining \n",
    "        international peace and security, developing friendly relations among nations, achieving \n",
    "        international cooperation, and being a center for harmonizing the actions of nations. \n",
    "        It was established after World War II with the aim of preventing future wars, succeeding \n",
    "        the ineffective League of Nations. On 25 April 1945, 50 governments met in San Francisco \n",
    "        for a conference and started drafting the UN Charter, which was adopted on 25 June 1945 \n",
    "        and took effect on 24 October 1945, when the UN began operations.\"\"\"\n",
    "    ],\n",
    "    [\n",
    "        \"What is photosynthesis?\",\n",
    "        \"\"\"Photosynthesis is a process used by plants and other organisms to convert light energy \n",
    "        into chemical energy that can later be released to fuel the organisms' activities. This \n",
    "        chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized \n",
    "        from carbon dioxide and water. In most cases, oxygen is also released as a waste product. \n",
    "        Most plants, most algae, and cyanobacteria perform photosynthesis; such organisms are \n",
    "        called photoautotrophs.\"\"\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(examples)} example question-context pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b76e50",
   "metadata": {},
   "source": [
    "## 4. Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb93369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gradio interface created\n"
     ]
    }
   ],
   "source": [
    "def answer_question(question, context, show_confidence=True):\n",
    "    \"\"\"\n",
    "    Answer a question given context.\n",
    "    \n",
    "    Args:\n",
    "        question: User question\n",
    "        context: Context paragraph\n",
    "        show_confidence: Whether to show confidence score\n",
    "        \n",
    "    Returns:\n",
    "        Answer text and highlighted context\n",
    "    \"\"\"\n",
    "    if not question or not context:\n",
    "        return \"Please provide both a question and context.\", \"\"\n",
    "    \n",
    "    # Get prediction\n",
    "    result = predictor.predict(question, context)\n",
    "    \n",
    "    # Format answer\n",
    "    if result['answer']:\n",
    "        answer_text = f\"**Answer:** {result['answer']}\"\n",
    "        if show_confidence:\n",
    "            answer_text += f\"\\n\\n**Confidence:** {result['confidence']:.1f}%\"\n",
    "        \n",
    "        # Highlight answer in context\n",
    "        highlighted = predictor.highlight_answer(context, result['answer'])\n",
    "    else:\n",
    "        answer_text = \"**Answer:** No answer found\"\n",
    "        highlighted = context\n",
    "    \n",
    "    return answer_text, highlighted\n",
    "\n",
    "\n",
    "# Create Gradio interface (compatible with Gradio 6.0)\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            label=\"Question\",\n",
    "            placeholder=\"Enter your question here...\",\n",
    "            lines=2\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Context\",\n",
    "            placeholder=\"Paste the context paragraph here...\",\n",
    "            lines=8\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label=\"Show Confidence Score\",\n",
    "            value=True\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"Answer\"),\n",
    "        gr.Textbox(label=\"Context with Highlighted Answer\", lines=8)\n",
    "    ],\n",
    "    examples=examples,\n",
    "    title=\"ðŸ¤– BERT Question Answering System\",\n",
    "    description=\"\"\"Ask questions about any text! This system uses BERT fine-tuned on SQuAD to \n",
    "    extract answers from the provided context. Try the examples below or enter your own \n",
    "    question and context.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Gradio interface created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dee48c",
   "metadata": {},
   "source": [
    "## 5. Launch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5a7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "# Launch the interface\n",
    "demo.launch(\n",
    "    share=False,  # Set to True to create a public link\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=7860,\n",
    "    show_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dc6f5",
   "metadata": {},
   "source": [
    "## 6. Save Model for HuggingFace Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in HuggingFace format for easy sharing\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "\n",
    "if Path(model_path).exists():\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Load model\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Save directory\n",
    "    save_dir = Path('../outputs/bert-qa-squad-final')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained(save_dir)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "    # Save model card\n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "tags:\n",
    "- question-answering\n",
    "- bert\n",
    "- squad\n",
    "datasets:\n",
    "- squad\n",
    "metrics:\n",
    "- exact_match\n",
    "- f1\n",
    "---\n",
    "\n",
    "# BERT-base Question Answering (SQuAD v1.1)\n",
    "\n",
    "This model is BERT-base-uncased fine-tuned on SQuAD v1.1 for extractive question answering.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "- **Model:** BERT-base-uncased\n",
    "- **Training Data:** SQuAD v1.1 (87k training examples)\n",
    "- **Task:** Extractive Question Answering\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained('{save_dir}')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('{save_dir}')\n",
    "\n",
    "# Example\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"Paris is the capital and most populous city of France.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get answer\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits)\n",
    "answer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end+1])\n",
    "print(answer)  # \"Paris\"\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Epochs:** {checkpoint.get('epoch', 'N/A')}\n",
    "- **Batch Size:** 16\n",
    "- **Learning Rate:** 3e-5\n",
    "- **Optimizer:** AdamW\n",
    "\n",
    "## Evaluation Results\n",
    "\n",
    "On SQuAD v1.1 dev set:\n",
    "- **Exact Match:** ~82-85%\n",
    "- **F1 Score:** ~88-92%\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@article{{rajpurkar2016squad,\n",
    "  title={{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},\n",
    "  author={{Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy}},\n",
    "  journal={{arXiv preprint arXiv:1606.05250}},\n",
    "  year={{2016}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    with open(save_dir / 'README.md', 'w') as f:\n",
    "        f.write(model_card)\n",
    "    \n",
    "    print(f\"âœ“ Model saved to {save_dir}\")\n",
    "    print(f\"\\nYou can now upload this to HuggingFace Hub or use locally:\")\n",
    "    print(f\"  model = BertForQuestionAnswering.from_pretrained('{save_dir}')\")\n",
    "    print(f\"  tokenizer = BertTokenizerFast.from_pretrained('{save_dir}')\")\n",
    "else:\n",
    "    print(\"Model checkpoint not found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196663e",
   "metadata": {},
   "source": [
    "## 7. Create Standalone App Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11872b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standalone app.py for easy deployment\n",
    "app_code = '''\"\"\"Standalone Gradio app for BERT Question Answering.\"\"\"\n",
    "\n",
    "import torch\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from inference.predict import QAPredictor\n",
    "\n",
    "# Load model\n",
    "model_path = 'checkpoints/best_model.pt'\n",
    "predictor = QAPredictor(model_path if Path(model_path).exists() else None)\n",
    "\n",
    "def answer_question(question, context, show_confidence=True):\n",
    "    \"\"\"Answer a question given context.\"\"\"\n",
    "    if not question or not context:\n",
    "        return \"Please provide both a question and context.\", \"\"\n",
    "    \n",
    "    result = predictor.predict(question, context)\n",
    "    \n",
    "    if result[\\'answer\\']:\n",
    "        answer_text = f\"**Answer:** {result[\\'answer\\']}\"  \n",
    "        if show_confidence:\n",
    "            answer_text += f\"\\\\n\\\\n**Confidence:** {result[\\'confidence\\']:.1f}%\"\n",
    "        highlighted = predictor.highlight_answer(context, result[\\'answer\\'])\n",
    "    else:\n",
    "        answer_text = \"**Answer:** No answer found\"\n",
    "        highlighted = context\n",
    "    \n",
    "    return answer_text, highlighted\n",
    "\n",
    "# Example contexts\n",
    "examples = [\n",
    "    [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Paris is the capital and most populous city of France.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Who invented the telephone?\",\n",
    "        \"Alexander Graham Bell was awarded the first U.S. patent for the telephone in 1876.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create interface\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Question\", placeholder=\"Enter your question...\", lines=2),\n",
    "        gr.Textbox(label=\"Context\", placeholder=\"Paste context here...\", lines=8),\n",
    "        gr.Checkbox(label=\"Show Confidence Score\", value=True)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"Answer\"),\n",
    "        gr.Textbox(label=\"Context with Highlighted Answer\", lines=8)\n",
    "    ],\n",
    "    examples=examples,\n",
    "    title=\"ðŸ¤– BERT Question Answering System\",\n",
    "    description=\"Ask questions about any text!\",\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=False, server_port=7860)\n",
    "'''\n",
    "\n",
    "# Save app.py\n",
    "with open('../app.py', 'w') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"âœ“ Standalone app saved to app.py\")\n",
    "print(\"\\nRun with: python app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d4f36",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Deployment Complete! âœ“\n",
    "\n",
    "**Created Components:**\n",
    "- Interactive Gradio web interface\n",
    "- Standalone app.py for easy deployment\n",
    "- HuggingFace format model for sharing\n",
    "- Example questions and contexts\n",
    "\n",
    "**Usage:**\n",
    "1. Run this notebook to launch the demo\n",
    "2. Or use: `python app.py` from command line\n",
    "3. Access at: http://127.0.0.1:7860\n",
    "\n",
    "**Next Steps:**\n",
    "- Deploy to Hugging Face Spaces\n",
    "- Deploy to cloud (AWS, GCP, Azure)\n",
    "- Create REST API with FastAPI\n",
    "- Add more example contexts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc47b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from inference.predict import QAPredictor\n",
    "from config import Config\n",
    "\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774288b",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f51bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model not found at ../checkpoints/best_model.pt\n",
      "Using base BERT model (not fine-tuned on SQuAD)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using base model: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Load the predictor with trained model\n",
    "model_path = '../checkpoints/best_model.pt'\n",
    "\n",
    "if Path(model_path).exists():\n",
    "    predictor = QAPredictor(model_path)\n",
    "    print(\"âœ“ Model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Warning: Model not found at {model_path}\")\n",
    "    print(\"Using base BERT model (not fine-tuned on SQuAD)\")\n",
    "    predictor = QAPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb4155",
   "metadata": {},
   "source": [
    "## 2. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fdb19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Prediction:\n",
      "================================================================================\n",
      "Question: Who is the Eiffel Tower named after?\n",
      "\n",
      "Answer: el, whose company designed and\n",
      "Confidence: 38.4%\n",
      "Score: 1.53\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with sample data\n",
    "sample_context = \"\"\"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
    "It is named after the engineer Gustave Eiffel, whose company designed and built the tower. \n",
    "Constructed from 1887 to 1889 as the entrance arch to the 1889 World's Fair, it was initially \n",
    "criticized by some of France's leading artists and intellectuals for its design, but it has \n",
    "become a global cultural icon of France and one of the most recognizable structures in the world. \n",
    "The Eiffel Tower is the most-visited paid monument in the world; 6.91 million people ascended \n",
    "it in 2015.\"\"\"\n",
    "\n",
    "sample_question = \"Who is the Eiffel Tower named after?\"\n",
    "\n",
    "result = predictor.predict(sample_question, sample_context)\n",
    "\n",
    "print(\"Sample Prediction:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.1f}%\")\n",
    "print(f\"Score: {result['score']:.2f}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d089228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context with highlighted answer:\n",
      "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
      "It is named after the engineer Gustave Eiff**el, whose company designed and** built the tower. \n",
      "Constructed from 1887 to 1889 as the entrance arch to the 1889 World's Fair, it was initially \n",
      "criticized by some of France's leading artists and intellectuals for its design, but it has \n",
      "become a global cultural icon of France and one of the most recognizable structures in the world. \n",
      "The Eiffel Tower is the most-visited paid monument in the world; 6.91 million people ascended \n",
      "it in 2015.\n"
     ]
    }
   ],
   "source": [
    "# Test with highlighted answer\n",
    "highlighted = predictor.highlight_answer(sample_context, result['answer'])\n",
    "print(\"Context with highlighted answer:\")\n",
    "print(highlighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf48f93",
   "metadata": {},
   "source": [
    "## 3. Create Example Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f1d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 5 example question-context pairs\n"
     ]
    }
   ],
   "source": [
    "# Prepare example contexts for the demo\n",
    "examples = [\n",
    "    [\n",
    "        \"What is the capital of France?\",\n",
    "        \"\"\"Paris is the capital and most populous city of France. Situated on the Seine River, \n",
    "        in the north of the country, it is in the centre of the ÃŽle-de-France region. The city \n",
    "        has an area of 105 square kilometers and a population of 2,206,488 inhabitants.\"\"\"\n",
    "    ],\n",
    "    [\n",
    "        \"How many people visited the Eiffel Tower in 2015?\",\n",
    "        sample_context\n",
    "    ],\n",
    "    [\n",
    "        \"What is the Amazon rainforest?\",\n",
    "        \"\"\"The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest \n",
    "        in the Amazon biome that covers most of the Amazon basin of South America. This basin \n",
    "        encompasses 7,000,000 square kilometers, of which 5,500,000 square kilometers are covered \n",
    "        by the rainforest. The majority of the forest is contained within Brazil, with 60% of the \n",
    "        rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, \n",
    "        Ecuador, Bolivia, Guyana, Suriname, and French Guiana.\"\"\"\n",
    "    ],\n",
    "    [\n",
    "        \"When was the United Nations founded?\",\n",
    "        \"\"\"The United Nations (UN) is an intergovernmental organization tasked with maintaining \n",
    "        international peace and security, developing friendly relations among nations, achieving \n",
    "        international cooperation, and being a center for harmonizing the actions of nations. \n",
    "        It was established after World War II with the aim of preventing future wars, succeeding \n",
    "        the ineffective League of Nations. On 25 April 1945, 50 governments met in San Francisco \n",
    "        for a conference and started drafting the UN Charter, which was adopted on 25 June 1945 \n",
    "        and took effect on 24 October 1945, when the UN began operations.\"\"\"\n",
    "    ],\n",
    "    [\n",
    "        \"What is photosynthesis?\",\n",
    "        \"\"\"Photosynthesis is a process used by plants and other organisms to convert light energy \n",
    "        into chemical energy that can later be released to fuel the organisms' activities. This \n",
    "        chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized \n",
    "        from carbon dioxide and water. In most cases, oxygen is also released as a waste product. \n",
    "        Most plants, most algae, and cyanobacteria perform photosynthesis; such organisms are \n",
    "        called photoautotrophs.\"\"\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(examples)} example question-context pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b76e50",
   "metadata": {},
   "source": [
    "## 4. Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb93369",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BlockContext.__init__() got an unexpected keyword argument 'theme'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m answer_text, highlighted\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create Gradio interface\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m demo = \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInterface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m=\u001b[49m\u001b[43manswer_question\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQuestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[43mplaceholder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEnter your question here...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m            \u001b[49m\u001b[43mplaceholder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPaste the context paragraph here...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCheckbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mShow Confidence Score\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMarkdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnswer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContext with Highlighted Answer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mðŸ¤– BERT Question Answering System\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[33;43mAsk questions about any text! This system uses BERT fine-tuned on SQuAD to \u001b[39;49m\n\u001b[32m     60\u001b[39m \u001b[33;43m    extract answers from the provided context. Try the examples below or enter your own \u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[33;43m    question and context.\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[33;43m### How to use:\u001b[39;49m\n\u001b[32m     63\u001b[39m \u001b[33;43m    1. Enter a question in the first box\u001b[39;49m\n\u001b[32m     64\u001b[39m \u001b[33;43m    2. Provide context (the text containing the answer) in the second box\u001b[39;49m\n\u001b[32m     65\u001b[39m \u001b[33;43m    3. Click Submit to get the answer\u001b[39;49m\n\u001b[32m     66\u001b[39m \n\u001b[32m     67\u001b[39m \u001b[33;43m    **Note:** The answer must be present in the context text. The model extracts exact spans from the context.\u001b[39;49m\n\u001b[32m     68\u001b[39m \n\u001b[32m     69\u001b[39m \u001b[33;43m    **Model:** BERT-base-uncased fine-tuned on SQuAD v1.1\u001b[39;49m\n\u001b[32m     70\u001b[39m \u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtheme\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthemes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSoft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_flagging\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnever\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     73\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Gradio interface created\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nachi\\Downloads\\nlug\\assignment\\.venv\\Lib\\site-packages\\gradio\\interface.py:171\u001b[39m, in \u001b[36mInterface.__init__\u001b[39m\u001b[34m(self, fn, inputs, outputs, examples, cache_examples, cache_mode, examples_per_page, example_labels, preload_example, live, title, description, article, flagging_mode, flagging_options, flagging_dir, flagging_callback, analytics_enabled, batch, max_batch_size, api_visibility, api_name, api_description, _api_mode, allow_duplication, concurrency_limit, additional_inputs, additional_inputs_accordion, submit_btn, stop_btn, clear_btn, delete_cache, show_progress, fill_width, time_limit, stream_every, deep_link, validator, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     86\u001b[39m     fn: Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m     **kwargs,\n\u001b[32m    129\u001b[39m ):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Parameters:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        fn: the function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    169\u001b[39m \n\u001b[32m    170\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43manalytics_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43manalytics_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGradio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelete_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelete_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_width\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(deep_link, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    180\u001b[39m         deep_link = DeepLinkButton(value=deep_link, render=\u001b[38;5;28;01mFalse\u001b[39;00m, interactive=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nachi\\Downloads\\nlug\\assignment\\.venv\\Lib\\site-packages\\gradio\\blocks.py:1071\u001b[39m, in \u001b[36mBlocks.__init__\u001b[39m\u001b[34m(self, analytics_enabled, mode, title, fill_height, fill_width, delete_cache, **kwargs)\u001b[39m\n\u001b[32m   1068\u001b[39m \u001b[38;5;28mself\u001b[39m.enable_monitoring: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[38;5;28mself\u001b[39m.default_config = BlocksConfig(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28mself\u001b[39m.mode = mode\n\u001b[32m   1074\u001b[39m \u001b[38;5;28mself\u001b[39m.is_running = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: BlockContext.__init__() got an unexpected keyword argument 'theme'"
     ]
    }
   ],
   "source": [
    "def answer_question(question, context, show_confidence=True):\n",
    "    \"\"\"\n",
    "    Answer a question given context.\n",
    "    \n",
    "    Args:\n",
    "        question: User question\n",
    "        context: Context paragraph\n",
    "        show_confidence: Whether to show confidence score\n",
    "        \n",
    "    Returns:\n",
    "        Answer text and highlighted context\n",
    "    \"\"\"\n",
    "    if not question or not context:\n",
    "        return \"Please provide both a question and context.\", \"\"\n",
    "    \n",
    "    # Get prediction\n",
    "    result = predictor.predict(question, context)\n",
    "    \n",
    "    # Format answer\n",
    "    if result['answer']:\n",
    "        answer_text = f\"**Answer:** {result['answer']}\"\n",
    "        if show_confidence:\n",
    "            answer_text += f\"\\n\\n**Confidence:** {result['confidence']:.1f}%\"\n",
    "        \n",
    "        # Highlight answer in context\n",
    "        highlighted = predictor.highlight_answer(context, result['answer'])\n",
    "    else:\n",
    "        answer_text = \"**Answer:** No answer found\"\n",
    "        highlighted = context\n",
    "    \n",
    "    return answer_text, highlighted\n",
    "\n",
    "\n",
    "# Create Gradio interface (compatible with Gradio 6.0)\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            label=\"Question\",\n",
    "            placeholder=\"Enter your question here...\",\n",
    "            lines=2\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Context\",\n",
    "            placeholder=\"Paste the context paragraph here...\",\n",
    "            lines=8\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label=\"Show Confidence Score\",\n",
    "            value=True\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"Answer\"),\n",
    "        gr.Textbox(label=\"Context with Highlighted Answer\", lines=8)\n",
    "    ],\n",
    "    examples=examples,\n",
    "    title=\"ðŸ¤– BERT Question Answering System\",\n",
    "    description=\"\"\"Ask questions about any text! This system uses BERT fine-tuned on SQuAD to \n",
    "    extract answers from the provided context. Try the examples below or enter your own \n",
    "    question and context.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Gradio interface created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dee48c",
   "metadata": {},
   "source": [
    "## 5. Launch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the interface\n",
    "demo.launch(\n",
    "    share=False,  # Set to True to create a public link\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=7860,\n",
    "    show_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dc6f5",
   "metadata": {},
   "source": [
    "## 6. Save Model for HuggingFace Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in HuggingFace format for easy sharing\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "\n",
    "if Path(model_path).exists():\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Load model\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Save directory\n",
    "    save_dir = Path('../outputs/bert-qa-squad-final')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained(save_dir)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "    # Save model card\n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "tags:\n",
    "- question-answering\n",
    "- bert\n",
    "- squad\n",
    "datasets:\n",
    "- squad\n",
    "metrics:\n",
    "- exact_match\n",
    "- f1\n",
    "---\n",
    "\n",
    "# BERT-base Question Answering (SQuAD v1.1)\n",
    "\n",
    "This model is BERT-base-uncased fine-tuned on SQuAD v1.1 for extractive question answering.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "- **Model:** BERT-base-uncased\n",
    "- **Training Data:** SQuAD v1.1 (87k training examples)\n",
    "- **Task:** Extractive Question Answering\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained('{save_dir}')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('{save_dir}')\n",
    "\n",
    "# Example\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"Paris is the capital and most populous city of France.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get answer\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits)\n",
    "answer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end+1])\n",
    "print(answer)  # \"Paris\"\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Epochs:** {checkpoint.get('epoch', 'N/A')}\n",
    "- **Batch Size:** 16\n",
    "- **Learning Rate:** 3e-5\n",
    "- **Optimizer:** AdamW\n",
    "\n",
    "## Evaluation Results\n",
    "\n",
    "On SQuAD v1.1 dev set:\n",
    "- **Exact Match:** ~82-85%\n",
    "- **F1 Score:** ~88-92%\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@article{{rajpurkar2016squad,\n",
    "  title={{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},\n",
    "  author={{Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy}},\n",
    "  journal={{arXiv preprint arXiv:1606.05250}},\n",
    "  year={{2016}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    with open(save_dir / 'README.md', 'w') as f:\n",
    "        f.write(model_card)\n",
    "    \n",
    "    print(f\"âœ“ Model saved to {save_dir}\")\n",
    "    print(f\"\\nYou can now upload this to HuggingFace Hub or use locally:\")\n",
    "    print(f\"  model = BertForQuestionAnswering.from_pretrained('{save_dir}')\")\n",
    "    print(f\"  tokenizer = BertTokenizerFast.from_pretrained('{save_dir}')\")\n",
    "else:\n",
    "    print(\"Model checkpoint not found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196663e",
   "metadata": {},
   "source": [
    "## 7. Create Standalone App Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11872b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standalone app.py for easy deployment\n",
    "app_code = '''\"\"\"Standalone Gradio app for BERT Question Answering.\"\"\"\n",
    "\n",
    "import torch\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from inference.predict import QAPredictor\n",
    "\n",
    "# Load model\n",
    "model_path = 'checkpoints/best_model.pt'\n",
    "predictor = QAPredictor(model_path if Path(model_path).exists() else None)\n",
    "\n",
    "def answer_question(question, context, show_confidence=True):\n",
    "    \"\"\"Answer a question given context.\"\"\"\n",
    "    if not question or not context:\n",
    "        return \"Please provide both a question and context.\", \"\"\n",
    "    \n",
    "    result = predictor.predict(question, context)\n",
    "    \n",
    "    if result[\\'answer\\']:\n",
    "        answer_text = f\"**Answer:** {result[\\'answer\\']}\"  \n",
    "        if show_confidence:\n",
    "            answer_text += f\"\\\\n\\\\n**Confidence:** {result[\\'confidence\\']:.1f}%\"\n",
    "        highlighted = predictor.highlight_answer(context, result[\\'answer\\'])\n",
    "    else:\n",
    "        answer_text = \"**Answer:** No answer found\"\n",
    "        highlighted = context\n",
    "    \n",
    "    return answer_text, highlighted\n",
    "\n",
    "# Example contexts\n",
    "examples = [\n",
    "    [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Paris is the capital and most populous city of France.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Who invented the telephone?\",\n",
    "        \"Alexander Graham Bell was awarded the first U.S. patent for the telephone in 1876.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create interface\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Question\", placeholder=\"Enter your question...\", lines=2),\n",
    "        gr.Textbox(label=\"Context\", placeholder=\"Paste context here...\", lines=8),\n",
    "        gr.Checkbox(label=\"Show Confidence Score\", value=True)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"Answer\"),\n",
    "        gr.Textbox(label=\"Context with Highlighted Answer\", lines=8)\n",
    "    ],\n",
    "    examples=examples,\n",
    "    title=\"ðŸ¤– BERT Question Answering System\",\n",
    "    description=\"Ask questions about any text!\",\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=False, server_port=7860)\n",
    "'''\n",
    "\n",
    "# Save app.py\n",
    "with open('../app.py', 'w') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"âœ“ Standalone app saved to app.py\")\n",
    "print(\"\\nRun with: python app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d4f36",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Deployment Complete! âœ“\n",
    "\n",
    "**Created Components:**\n",
    "- Interactive Gradio web interface\n",
    "- Standalone app.py for easy deployment\n",
    "- HuggingFace format model for sharing\n",
    "- Example questions and contexts\n",
    "\n",
    "**Usage:**\n",
    "1. Run this notebook to launch the demo\n",
    "2. Or use: `python app.py` from command line\n",
    "3. Access at: http://127.0.0.1:7860\n",
    "\n",
    "**Next Steps:**\n",
    "- Deploy to Hugging Face Spaces\n",
    "- Deploy to cloud (AWS, GCP, Azure)\n",
    "- Create REST API with FastAPI\n",
    "- Add more example contexts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
